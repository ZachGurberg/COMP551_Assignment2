{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Verify(expression: bool, message: str):\n",
    "    if not expression:\n",
    "        raise Exception(message)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 1.1: IMDB Data loading</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class IMDBSample:\n",
    "    def __init__(self, rowIndex, numberOfFeatures=89527):\n",
    "        self.rowIndex = rowIndex\n",
    "        self.features = np.zeros(numberOfFeatures)\n",
    "        self.label = -1\n",
    "\n",
    "class IMDBDataLoader:\n",
    "    def __init__(self, vocabFilepath, featFilepath):\n",
    "        self.samples = []\n",
    "        self.words = []\n",
    "        self.vocabFilepath = vocabFilepath\n",
    "        self.featFilepath = featFilepath\n",
    "        self.ParseIntoVectors()\n",
    "    \n",
    "    def ParseIntoVectors(self):\n",
    "        with open(self.featFilepath, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start=0):\n",
    "                currentSample = IMDBSample(line_number)\n",
    "                parts = line.split()\n",
    "                Verify(int(parts[0])>6 or int(parts[0]) < 6, \"Error: Rating value unexpected in IMDB dataloader.\")\n",
    "                currentSample.label = int(int(parts[0]))\n",
    "                for part in parts[1:]:\n",
    "                    wordIndex, frequency = map(int, part.split(':'))\n",
    "                    Verify(wordIndex<currentSample.features.size, \"Word index larger than number of features in IMDB dataloader\")\n",
    "                    Verify(frequency>=0, \"Word Frequency is smaller than expected in IMDB dataloader.\")\n",
    "                    currentSample.features[wordIndex] = frequency\n",
    "                self.samples.append(currentSample)\n",
    "\n",
    "        with open(self.vocabFilepath, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start = 0):\n",
    "                self.words.append(line)\n",
    "                \n",
    "    \n",
    "    def GetData(self):\n",
    "        numberOfSamples = len(self.samples)\n",
    "        numberOfFeatures = self.samples[0].features.size if self.samples else 0\n",
    "\n",
    "        X = np.zeros((numberOfSamples, numberOfFeatures))\n",
    "        y = np.zeros(numberOfSamples)\n",
    "\n",
    "        for i, sample in enumerate(self.samples):\n",
    "            X[i,:] = sample.features\n",
    "            y[i] = sample.label\n",
    "        return y, X\n",
    "    \n",
    "    def GetWords(self):\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabFilepath = '../aclImdb/imdb.vocab'\n",
    "trainfeatFilepath = '../aclImdb/train/labeledBow.feat'\n",
    "testfeatFilepath = '../aclImdb/test/labeledBow.feat'\n",
    "dataloaderTrain = IMDBDataLoader(vocabFilepath, trainfeatFilepath)\n",
    "dataloaderTest = IMDBDataLoader(vocabFilepath, testfeatFilepath)\n",
    "print(\"Dataloading complete\")\n",
    "y_train, X_train = dataloaderTrain.GetData()\n",
    "y_test, X_test = dataloaderTest.GetData()\n",
    "\n",
    "words = dataloaderTrain.GetWords()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data filtering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onePercentThreshold = int(y_train.size*0.01)\n",
    "fiftyPercentThreshold = int(y_train.size*0.5)\n",
    "featureFrequencies = np.sum(X_train > 0, axis=0)\n",
    "\n",
    "featuresToKeep = (featureFrequencies > onePercentThreshold) & (featureFrequencies < fiftyPercentThreshold)\n",
    "featureFrequenciesFiltered = featureFrequencies[featuresToKeep]\n",
    "XFiltered_train = X_train[:, featuresToKeep]\n",
    "XFiltered_test = X_test[:, featuresToKeep]\n",
    "wordsFiltered = [word for word, keep in zip(words, featuresToKeep) if keep]\n",
    "\n",
    "weights = np.linalg.inv(XFiltered_train.T @ XFiltered_train) @ XFiltered_train.T @ y_train #OLS SSE Solution\n",
    "D_Selected = 1000\n",
    "feature_importances = np.abs(np.copy(weights))\n",
    "top_features_indices = np.argsort(feature_importances)[-D_Selected:]\n",
    "XFiltered_train_selected = XFiltered_train[:, top_features_indices]\n",
    "XFiltered_test_selected = XFiltered_test[:, top_features_indices]\n",
    "featureFrequenciesFiltered_Selected = featureFrequenciesFiltered[top_features_indices]\n",
    "wordsFiltered_selected = [wordsFiltered[i] for i in top_features_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Displaying Top Words (Task 3.1)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfWordsToDisplay = 10 #both positive and negative side\n",
    "positive_weights_indices = np.argsort(weights)[-numberOfWordsToDisplay:]\n",
    "negative_weights_indices = np.argsort(weights)[:numberOfWordsToDisplay]\n",
    "words_with_largest_positive_weights = [(wordsFiltered[i].strip(), weights[i], featureFrequenciesFiltered[i]) for i in positive_weights_indices]\n",
    "words_with_largest_negative_weights = [(wordsFiltered[i].strip(), weights[i], featureFrequenciesFiltered[i]) for i in negative_weights_indices]\n",
    "\n",
    "print(f\"Words with the {numberOfWordsToDisplay} largest positive weights:\")\n",
    "for word, weight, frequency in reversed(words_with_largest_positive_weights):\n",
    "    print(f\"Word: '{word}', Weight: {weight:.5f}, Frequency: {frequency}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Words with the {numberOfWordsToDisplay} largest negative weights:\")\n",
    "for word, weight, frequency in words_with_largest_negative_weights:\n",
    "    print(f\"Word: '{word}', Weight: {weight:.5f}, Frequency: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 2.1 Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CE_Loss:\n",
    "    def set_data(self, y_true, X):\n",
    "        self.y_true = y_true\n",
    "        self.X = X\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        a = self.X@w\n",
    "        # y_pred = 1/(1+np.exp(-a))\n",
    "        # return -np.mean(self.y_true*np.log(y_pred)-(1-self.y_true)*np.log(1-y_pred))\n",
    "        ###Equivalently for precision and numerical stability (slide 9/37 logistic regression)###\n",
    "        return np.sum(self.y_true*np.log1p(np.exp(-a)) +(1-self.y_true)*np.log1p(np.exp(a)))\n",
    "    \n",
    "    def grad(self, w):\n",
    "        a = self.X@w\n",
    "        y_pred = 1/(1+np.exp(-a))\n",
    "        return self.X.T@(y_pred-self.y_true)/self.y_true.size\n",
    "        \n",
    "#minimizes\n",
    "class Gradient_Descent:\n",
    "    def __init__(self, weights, learning_rate, max_iter, epsilon, cost_function):\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.cost_acc = []\n",
    "        self.epsilon = epsilon\n",
    "        self.f = cost_function\n",
    "\n",
    "    def optimize(self):\n",
    "        initial_cost = self.f(self.weights)\n",
    "        self.cost_acc.append(initial_cost)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.weights -= self.f.grad(self.weights)\n",
    "            new_cost = self.f(self.weights)\n",
    "            print(f\"Iteration{i+1} \\t Cost: {new_cost}\") #Toggle\n",
    "            self.cost_acc.append(new_cost)\n",
    "            delta = new_cost-self.cost_acc[i]\n",
    "            if (np.abs(delta)<self.epsilon):\n",
    "                return new_cost\n",
    "        return new_cost\n",
    "            \n",
    "\n",
    "class Binary_Logitstic_Regression:\n",
    "    def __init__(self, learning_rate = 0.1, max_iter=1000, epsilon=1e-6, cost_function=CE_Loss()):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "        self.f = cost_function\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = (y>5).astype(int) #Turn to binary immediately\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "        self.f.set_data(self.y, self.X)\n",
    "        optimizer = Gradient_Descent(self.weights, self.learning_rate, self.max_iter, self.epsilon, self.f)\n",
    "        optimizer.optimize()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Verify(self.weights.shape[0]>0, \"Weights not optimized yet\")\n",
    "        return 1/(1+np.exp(-X@self.weights))\n",
    "    \n",
    "def perform_roc_and_auroc(predicted_probabilities, true_binary_labels):\n",
    "    if predicted_probabilities.shape != true_binary_labels.shape:\n",
    "        raise ValueError(\"Shapes of predicted probabilities and true labels must match.\")\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(true_binary_labels, predicted_probabilities)\n",
    "    \n",
    "    # Calculate the AUROC\n",
    "    auroc = auc(fpr, tpr)\n",
    "    roc_curve_data = (fpr, tpr, thresholds)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auroc)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return auroc, roc_curve_data\n",
    "\n",
    "    \n",
    "\n",
    "regressionObject = Binary_Logitstic_Regression()\n",
    "regressionObject.fit(XFiltered_train_selected, y_train)\n",
    "test_output = regressionObject.predict(XFiltered_test_selected)\n",
    "auroc, roc_curve_data = perform_roc_and_auroc(test_output, (y_test>5).astype(int))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrITPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
