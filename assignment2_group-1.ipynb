{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Verify(expression: bool, message: str):\n",
    "    if not expression:\n",
    "        raise Exception(message)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 1.1: IMDB Data loading</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class IMDBSample:\n",
    "    def __init__(self, rowIndex, numberOfFeatures=89527):\n",
    "        self.rowIndex = rowIndex\n",
    "        self.features = np.zeros(numberOfFeatures)\n",
    "        self.label = -1\n",
    "\n",
    "class IMDBDataLoader:\n",
    "    def __init__(self, vocabFilepath, featFilepath):\n",
    "        self.samples = []\n",
    "        self.words = []\n",
    "        self.vocabFilepath = vocabFilepath\n",
    "        self.featFilepath = featFilepath\n",
    "        self.ParseIntoVectors()\n",
    "    \n",
    "    def ParseIntoVectors(self):\n",
    "        with open(self.featFilepath, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start=0):\n",
    "                currentSample = IMDBSample(line_number)\n",
    "                parts = line.split()\n",
    "                Verify(int(parts[0])>6 or int(parts[0]) < 6, \"Error: Rating value unexpected in IMDB dataloader.\")\n",
    "                currentSample.label = int(int(parts[0]))\n",
    "                for part in parts[1:]:\n",
    "                    wordIndex, frequency = map(int, part.split(':'))\n",
    "                    Verify(wordIndex<currentSample.features.size, \"Word index larger than number of features in IMDB dataloader\")\n",
    "                    Verify(frequency>=0, \"Word Frequency is smaller than expected in IMDB dataloader.\")\n",
    "                    currentSample.features[wordIndex] = frequency\n",
    "                self.samples.append(currentSample)\n",
    "\n",
    "        with open(self.vocabFilepath, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start = 0):\n",
    "                self.words.append(line)\n",
    "                \n",
    "    \n",
    "    def GetData(self):\n",
    "        numberOfSamples = len(self.samples)\n",
    "        numberOfFeatures = self.samples[0].features.size if self.samples else 0\n",
    "\n",
    "        X = np.zeros((numberOfSamples, numberOfFeatures))\n",
    "        y = np.zeros(numberOfSamples)\n",
    "\n",
    "        for i, sample in enumerate(self.samples):\n",
    "            X[i,:] = sample.features\n",
    "            y[i] = sample.label\n",
    "        return y, X\n",
    "    \n",
    "    def GetWords(self):\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabFilepath = '../aclImdb/imdb.vocab'\n",
    "trainfeatFilepath = '../aclImdb/train/labeledBow.feat'\n",
    "testfeatFilepath = '../aclImdb/test/labeledBow.feat'\n",
    "dataloaderTrain = IMDBDataLoader(vocabFilepath, trainfeatFilepath)\n",
    "dataloaderTest = IMDBDataLoader(vocabFilepath, testfeatFilepath)\n",
    "print(\"Dataloading complete\")\n",
    "y_train, X_train = dataloaderTrain.GetData()\n",
    "y_test, X_test = dataloaderTest.GetData()\n",
    "\n",
    "words = dataloaderTrain.GetWords()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data filtering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onePercentThreshold = int(y_train.size*0.01)\n",
    "fiftyPercentThreshold = int(y_train.size*0.5)\n",
    "featureFrequencies = np.sum(X_train > 0, axis=0)\n",
    "\n",
    "featuresToKeep = (featureFrequencies > onePercentThreshold) & (featureFrequencies < fiftyPercentThreshold)\n",
    "featureFrequenciesFiltered = featureFrequencies[featuresToKeep]\n",
    "XFiltered_train = X_train[:, featuresToKeep]\n",
    "XFiltered_test = X_test[:, featuresToKeep]\n",
    "wordsFiltered = [word for word, keep in zip(words, featuresToKeep) if keep]\n",
    "\n",
    "weights = np.linalg.inv(XFiltered_train.T @ XFiltered_train) @ XFiltered_train.T @ y_train #OLS SSE Solution\n",
    "D_Selected = 1000\n",
    "feature_importances = np.abs(np.copy(weights))\n",
    "top_features_indices = np.argsort(feature_importances)[-D_Selected:]\n",
    "XFiltered_train_selected = XFiltered_train[:, top_features_indices]\n",
    "XFiltered_test_selected = XFiltered_test[:, top_features_indices]\n",
    "featureFrequenciesFiltered_Selected = featureFrequenciesFiltered[top_features_indices]\n",
    "wordsFiltered_selected = [wordsFiltered[i] for i in top_features_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Displaying Top Words (Task 3.1)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfWordsToDisplay = 10 #both positive and negative side\n",
    "positive_weights_indices = np.argsort(weights)[-numberOfWordsToDisplay:]\n",
    "negative_weights_indices = np.argsort(weights)[:numberOfWordsToDisplay]\n",
    "words_with_largest_positive_weights = [(wordsFiltered[i].strip(), weights[i], featureFrequenciesFiltered[i]) for i in positive_weights_indices]\n",
    "words_with_largest_negative_weights = [(wordsFiltered[i].strip(), weights[i], featureFrequenciesFiltered[i]) for i in negative_weights_indices]\n",
    "\n",
    "print(f\"Words with the {numberOfWordsToDisplay} largest positive weights:\")\n",
    "for word, weight, frequency in reversed(words_with_largest_positive_weights):\n",
    "    print(f\"Word: '{word}', Weight: {weight:.5f}, Frequency: {frequency}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Words with the {numberOfWordsToDisplay} largest negative weights:\")\n",
    "for word, weight, frequency in words_with_largest_negative_weights:\n",
    "    print(f\"Word: '{word}', Weight: {weight:.5f}, Frequency: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CE_Loss:\n",
    "    def set_data(self, y_true, X):\n",
    "        self.y_true = y_true\n",
    "        self.X = X\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        a = self.X@w\n",
    "        # y_pred = 1/(1+np.exp(-a))\n",
    "        # return -np.mean(self.y_true*np.log(y_pred)-(1-self.y_true)*np.log(1-y_pred))\n",
    "        ###Equivalently for precision and numerical stability (slide 9/37 logistic regression)###\n",
    "        return np.sum(self.y_true*np.log1p(np.exp(-a)) +(1-self.y_true)*np.log1p(np.exp(a)))\n",
    "    \n",
    "    def grad(self, w):\n",
    "        a = self.X@w\n",
    "        y_pred = 1/(1+np.exp(-a))\n",
    "        return self.X.T@(y_pred-self.y_true)/self.y_true.size\n",
    "        \n",
    "#minimizes\n",
    "class Gradient_Descent:\n",
    "    def __init__(self, weights, learning_rate, max_iter, epsilon, cost_function):\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.cost_acc = []\n",
    "        self.epsilon = epsilon\n",
    "        self.f = cost_function\n",
    "\n",
    "    def optimize(self):\n",
    "        initial_cost = self.f(self.weights)\n",
    "        self.cost_acc.append(initial_cost)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.weights -= self.f.grad(self.weights)\n",
    "            new_cost = self.f(self.weights)\n",
    "            print(f\"Iteration{i+1} \\t Cost: {new_cost}\") #Toggle\n",
    "            self.cost_acc.append(new_cost)\n",
    "            delta = new_cost-self.cost_acc[i]\n",
    "            if (np.abs(delta)<self.epsilon):\n",
    "                return new_cost\n",
    "        return new_cost\n",
    "            \n",
    "\n",
    "class Binary_Logitstic_Regression:\n",
    "    def __init__(self, learning_rate = 0.1, max_iter=1000, epsilon=1e-6, cost_function=CE_Loss()):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "        self.f = cost_function\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = (y>5).astype(int) #Turn to binary immediately\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "        self.f.set_data(self.y, self.X)\n",
    "        optimizer = Gradient_Descent(self.weights, self.learning_rate, self.max_iter, self.epsilon, self.f)\n",
    "        optimizer.optimize()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Verify(self.weights.shape[0]>0, \"Weights not optimized yet\")\n",
    "        return 1/(1+np.exp(-X@self.weights))\n",
    "    \n",
    "def perform_roc_and_auroc(predicted_probabilities, true_binary_labels):\n",
    "    if predicted_probabilities.shape != true_binary_labels.shape:\n",
    "        raise ValueError(\"Shapes of predicted probabilities and true labels must match.\")\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(true_binary_labels, predicted_probabilities)\n",
    "    \n",
    "    # Calculate the AUROC\n",
    "    auroc = auc(fpr, tpr)\n",
    "    roc_curve_data = (fpr, tpr, thresholds)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auroc)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return auroc, roc_curve_data\n",
    "\n",
    "    \n",
    "\n",
    "regressionObject = Binary_Logitstic_Regression()\n",
    "regressionObject.fit(XFiltered_train_selected, y_train)\n",
    "test_output = regressionObject.predict(XFiltered_test_selected)\n",
    "auroc, roc_curve_data = perform_roc_and_auroc(test_output, (y_test>5).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 1.2: 20 News Groups </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearing all local variables\n",
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "#also the one hot encoding template\n",
    "categories = ['rec.autos', 'rec.sport.hockey', 'sci.med', 'talk.politics.mideast', 'comp.graphics']\n",
    "\n",
    "bunchTrain = datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "bunchTest = datasets.fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "y_train = bunchTrain.target\n",
    "y_test = bunchTest.target\n",
    "\n",
    "# print(len(bunchTrain.data)) #2936\n",
    "# print(len(bunchTest.data)) #1956\n",
    "\n",
    "#We now need to convert the text data from bunch[Train or Test].data to a vector representation \n",
    "#Following guide in https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#We first tokenize with CountVectorizer so that we have vectors of the occurrences of the different words\n",
    "#The index of the value (occurrence) in the feature vector for a given sample is associated to the order of appearance of a certain word over all samples\n",
    "#So if we have three samples [\"Hello my name\"] [\"is zach and\"] [\"i like chicken\"]\n",
    "#Our feature vectors each be of length 9 where the indices correspond to the occurence of [\"hello\", \"my\", \"name\", \"is\", \"zach\", \"and\", \"i\", \"like\",\"chicken\"]\n",
    "#So our samples will be [1,1,1,0,0,0,0,0,], [0,0,0,1,1,1,0,0,0] and [0,0,0,0,0,0,1,1,1]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(bunchTrain.data) #Count vectorizer also supposedly filters stopwords\n",
    "X_test_counts = count_vect.transform(bunchTest.data) #We transform from the fitted transform on the training data so that our test data feature vectors are the same encoding - ie use the same index-word pairings. Otherwise the weights will not be aligned to the same words\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(X_test_counts.shape)\n",
    "\n",
    "#Now we the same 1% and 50% thresholds to reduce our dimensionality\n",
    "#binary vect in NxD but binary (1 means word is in sample, 0 means no)\n",
    "binary_vect = CountVectorizer(binary=True)\n",
    "X_train_binary = binary_vect.fit_transform(bunchTrain.data)\n",
    "wordFrequencies = np.asarray(np.sum(X_train_binary,axis=0)).squeeze() #ie number of samples containing a word\n",
    "onePercentThreshold = 0.01*y_train.shape[0]\n",
    "fiftyPercentThreshold = 0.50*y_train.shape[0]\n",
    "featuresToKeep = np.where((wordFrequencies > onePercentThreshold) & (wordFrequencies < fiftyPercentThreshold)\n",
    ")[0]\n",
    "X_train_filtered = X_train_counts[:,featuresToKeep]\n",
    "X_test_filtered = X_test_counts[:,featuresToKeep]\n",
    "words_filtered = [word for word, idx in count_vect.vocabulary_.items() if idx in featuresToKeep]\n",
    "\n",
    "print(X_train_filtered.shape)\n",
    "print(X_test_filtered.shape)\n",
    "print(len(words_filtered))\n",
    "\n",
    "#Now to get the most important K features, we can use SelectKBest and mutual_info_calssif\n",
    "#View the tutorial linked above (scikit)\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "k = 1000 #Number of final dimensions\n",
    "selector = SelectKBest(mutual_info_classif, k=k)\n",
    "selector.fit(X_train_filtered, y_train)\n",
    "\n",
    "X_train_selected = selector.transform(X_train_filtered)\n",
    "X_test_selected = selector.transform(X_test_filtered)\n",
    "\n",
    "selected_features_mask = selector.get_support(indices=True)\n",
    "words_selected = [words_filtered[i] for i in selected_features_mask]\n",
    "\n",
    "print(X_train_selected.shape)\n",
    "print(X_test_selected.shape)\n",
    "print(len(words_selected))\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "#Now we apply one hot encoding\n",
    "num_classes = len(categories)\n",
    "y_train_encoded = np.zeros((y_train.size, num_classes)) #initializing empty matrices\n",
    "y_test_encoded = np.zeros((y_test.size, num_classes))\n",
    "y_train_encoded[np.arange(y_train.size), y_train] =1 #populating\n",
    "y_test_encoded[np.arange(y_test.size), y_test] =1\n",
    "\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-class Regression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as earlier, just copied so we don't have to rerun every cell everytime\n",
    "class Gradient_Descent:\n",
    "    def __init__(self, weights, learning_rate, max_iter, epsilon, cost_function):\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.cost_acc = []\n",
    "        self.epsilon = epsilon\n",
    "        self.f = cost_function\n",
    "\n",
    "    def optimize(self):\n",
    "        initial_cost = self.f(self.weights)\n",
    "        self.cost_acc.append(initial_cost)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.weights -= self.f.grad(self.weights)\n",
    "            new_cost = self.f(self.weights)\n",
    "            print(f\"Iteration{i+1} \\t Cost: {new_cost}\") #Toggle\n",
    "            self.cost_acc.append(new_cost)\n",
    "            delta = new_cost-self.cost_acc[i]\n",
    "            if (np.abs(delta)<self.epsilon):\n",
    "                return new_cost\n",
    "        return new_cost\n",
    "\n",
    "class MultiClass_CE_Loss:\n",
    "    def set_data(self, X, y_true, numerical_stability = 1e-9):\n",
    "        #X is NxD\n",
    "        #y_true is NxC (one hot encoded)\n",
    "        self.X = X\n",
    "        self.y_true = y_true\n",
    "        self.numerical_stability = numerical_stability\n",
    "\n",
    "    #Softmax implementation uses a numerical stabilitiy trick of subtracting the maximum logit from each\n",
    "    #https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "    #Found online\n",
    "    def softmax(self, logits):\n",
    "        #Logits is NxC\n",
    "        expLogits = np.exp(logits-np.max(logits, axis=1, keepdims=True)) #NxC\n",
    "        sumExp = np.sum(expLogits, axis=1, keepdims=True) #Nx1\n",
    "        probabilities = expLogits/sumExp #NxC\n",
    "        return probabilities #NxC\n",
    "    \n",
    "    #Evaluates cross entropy from the self.X, self.y_true and the weights input as argument\n",
    "    def __call__(self, w):\n",
    "        #W is DxC\n",
    "        logits = self.X@w #NxC\n",
    "        probabilities = self.softmax(logits) #NxC\n",
    "        cross_entropies = -np.sum(self.y_true * np.log(probabilities + self.numerical_stability), axis=1) #Nx1\n",
    "        return np.sum(cross_entropies) #1x1\n",
    "    \n",
    "    #Gets probabilities from an X input (used for the test dataset)\n",
    "    def get_probabilities(self, X, w):\n",
    "        logits = X@w #NxC\n",
    "        probabilities = self.softmax(logits) #NxC\n",
    "        return probabilities\n",
    "    \n",
    "    #Returns the gradient from the self.x, self.y_true and the weights input as argument\n",
    "    def grad(self, w):\n",
    "        logits = self.X@w #NxC\n",
    "        probabilities = self.softmax(logits) #NxC\n",
    "        y_pred_minus_y_true = probabilities - self.y_true #element wise, NxC\n",
    "        return self.X.T@(y_pred_minus_y_true)/self.X.shape[0] #DxN * NxC = DxC, averaged gradient rather than sum of gradient at each sample\n",
    "    \n",
    "class MultiClass_Regression:\n",
    "    def __init__(self, learning_rate = 0.1, max_iter=15000, epsilon=1e-6, cost_function=MultiClass_CE_Loss()):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "        self.f = cost_function\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.ones((X.shape[1], y.shape[1],), dtype=float) #DxC\n",
    "        self.f.set_data(X, y)\n",
    "        optimizer = Gradient_Descent(self.weights, self.learning_rate, self.max_iter, self.epsilon, self.f)\n",
    "        optimizer.optimize()\n",
    "    \n",
    "    #Returns NxC probabilities matrix\n",
    "    def predict(self, X):\n",
    "        assert self.weights.shape[0]>0, \"Weights not optimized yet\"\n",
    "        return self.f.get_probabilities(X, self.weights)\n",
    "    \n",
    "#Proababilities NxC, y_true NxC one hot encoded\n",
    "def compute_classification_accuracy(probabilities, y_true):\n",
    "    predicted_classes = np.argmax(probabilities, axis=1) #Nx1 (0,1,2,3,4, etc)\n",
    "    true_classes = np.argmax(y_true, axis=1) #Nx1 (0,1,2,3,4,etc)\n",
    "    correct_predictions = np.sum(predicted_classes == true_classes) #1x1\n",
    "    total_samples = probabilities.shape[0] #N\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy*100 #returns as percentage, not decimal\n",
    "\n",
    "multiclassregression = MultiClass_Regression()\n",
    "multiclassregression.fit(X_train_selected, y_train_encoded)\n",
    "predictions = multiclassregression.predict(X_test_selected)\n",
    "accuracy = compute_classification_accuracy(predictions, y_test_encoded)\n",
    "print(f\"Classification Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrITPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
